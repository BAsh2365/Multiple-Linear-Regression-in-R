---
title: "Predicting Atlanta Falcons NFL Touchdowns with Regression Modelling"
author: "Bhargav Ashok"
date: "2025-05-16"
output: 
  pdf_document
---

### 0. Setup - Install Packages: tidyr, readxl, dplyr, car, glmnet, Metrics, 3dscatterplot

```{r setup, include=FALSE}
# hide setup output
suppressPackageStartupMessages({
  library(tidyr)
  library(readxl)
  library(dplyr)
  library(car)
  library(glmnet)
  library(Metrics)
  library(scatterplot3d)
})

knitr::opts_chunk$set(
  echo       = TRUE,      # donâ€™t show code by default
  message    = FALSE,      # suppress messages
  warning    = FALSE,      # suppress warnings
  fig.width  = 6,          # width in inches
  fig.height = 4,          # height in inches
  fig.align  = "center",   # center all figures
  out.width  = "80%"       # scale figures to 80% of page width
)
```

### Loading our Atlanta Falcons Data

```{r}
Atlanta_Falcons_data <- read_excel("Atlanta_Falcons_data.xlsx")
head(Atlanta_Falcons_data)
```

### Preparing our data, cleaning extensively and training

Here, we are cleaning our data with the tidyr package, filtering by specific offensive positions and then splitting our data into a trained and tested set.

```{r}
vars_needed <- c("TD", "Tgt", "Rec", "Ctch%", "Yds")

offensive_positions <- c("QB", "RB", "WR", "TE")

clean_df <- Atlanta_Falcons_data %>%
  drop_na(all_of(vars_needed)) %>%
  filter(Pos %in% offensive_positions)

set.seed(123)
n       <- nrow(clean_df)
train_i <- sample(n, 0.8*n)
train_df <- clean_df[ train_i, ]
test_df  <- clean_df[-train_i, ]
```

### Fitting our Model (regular MLR)

Next, we fit our model using the equation: Y = (Beta)0 + (Beta)1X1 + (Beta)2X2 + ... + (Beta)nXn + error (assuming normal distribution)

```{r}
mlr_fit <- lm((TD) ~ Tgt + Rec + `Ctch%` + Yds, data = train_df)
summary(mlr_fit)

```

The summary statistics are displayed above. Note the coefficients for the MLR equation and the adjusted R-squared Value.

The MLR equation can be identified and written as: "TD = -0.531 + 0.028*Tgt + 0.013*Rec + 0.415*Ctch% + 0.009*Yds".

This equation suggests that Catch percentage has the largest positive effect on Touchdowns, with Receptions being second, followed by Yards.

### Data Visualization of the Matrix Scatterplot
Here we see that the scatter plot compares all the different variables together to check
for linearity and how each variable influences each other. We may see non-linearity
between some variables but that will be accounted for later on.
```{r}
pairs(
  ~ TD + Tgt + Rec + `Ctch%` + Yds,
  data = train_df,
  main = "Matrix Scatterplot of MLR Variables",
  col = "red",
  pch = 19
)
```

### Data visualization of the 3d scatterplot

Here we are taking the top two linearly correlated variables (not largest positive effect) and plotting them on a 3d scatterplot using the R 3d scatterplot package.
```{r}
scatterplot3d(
  x = train_df$Tgt,
  y = train_df$TD,
  z = train_df$Yds,
  main = "3D Scatterplot: TD ~ Tgt + Yds",
  xlab = "Targets",
  ylab = "Touchdowns",
  zlab = "Yards",
  color = "green",
  pch = 19
)

```

### MLR fit data display (looking at model data)

Then, we display plots for our fitted data (qqnorm and histogram of residuals)
```{r}
hist(resid(mlr_fit), main="OLS Train Residuals", xlab="Residual")
qqnorm(resid(mlr_fit)); qqline(resid(mlr_fit))
```

From the plots, we see that our trained residual data is roughly normal along with the qqplot being roughly normal as well (with deviation in the extreme values or outliers).

### test data display (looking at unseen test data)

After that, we look at our unseen test data plots.
```{r}
test_df$pred_ols  <- predict(mlr_fit, newdata = test_df)
resid_test_ols    <- test_df$TD - test_df$pred_ols

# Plots
hist(resid_test_ols, main="OLS Test Residuals", xlab="Residual")
qqnorm(resid_test_ols); qqline(resid_test_ols)

plot(
  test_df$pred_ols, test_df$TD,
  xlab="Predicted TD (OLS)", ylab="Actual TD",
  main="Test: OLS Pred vs Actual", pch=19
)
abline(0,1,col="blue")

```

We see the linear regression line on our predicted test data above along with the qqnorm and residual plots. While normality and non-skewed data is important in general, the trained data is a more accurate representation of it compared to the tested data (as the model uses the trained data for statistical analysis) so the plots here don't matter as much for inference (but it can help us check how our ML model is performing). 


### Using Cross validation + Ridge/Lasso Regression
```{r}
vif(mlr_fit)

```

Our VIF (Variance Inflation Factor, found using car package in R) is displayed above. An accurate explanation from Statsmodels can be found here:

"The variance inflation factor is a measure for the increase of the variance of the parameter estimates if an additional variable is added to linear regression. It is a measure for multicollinearity of the design matrix".

We see that the VIF \> 10 (a rule of thumb is that if the VF is greater than 5-10, it indicates high multicollinearity). This is not ideal for this scenario because the variables need to be standardized to account for the number inflation in multicollinearity (since they are supposed to be statistically significant, which is not reflected in the regular MLR). 

There is also some concern of Heteroscedasticity (not a contsant variance across residuals)
```{r}
par(mfrow = c(1, 2))           

plot(mlr_fit, which = 1) 

```
The plot above shows some deviation to the constant horizontal line (indicating non constant variance).

Due to this, we need to switch to a new type of regression model for even more accurate results. We use a technique called K-folds cross validation, where the data is split into multiple subsets and is iterated more than once in order to account for the multicollinearity inflation which is indicated above, as well improving the model to see how accurately it can predict unseen data points. This involves putting our train/test data into matrices, and then running regularized models called Ridge and Lasso regression respectively. 

Lasso regression accounts for the absolute value of the important coefficients and shrinks them using a penalty factor. Ridge regression accounts for the squared value of the coefficients and shrinks them with a similar penalty factor (all in the means of regularizing our data (also called hyper parameter tuning)). We use the glmnet package for this.

```{r}
# Prepare matrices
x_train <- model.matrix(TD~Tgt+Rec+`Ctch%`+Yds, train_df)[,-1]
y_train <- train_df$TD
x_test  <- model.matrix(TD~Tgt+Rec+`Ctch%`+Yds, test_df)[,-1]
y_test  <- test_df$TD

# Ridge
cv_ridge  <- cv.glmnet(x_train,y_train,alpha=0)
best_ridge<- cv_ridge$lambda.min
ridge_mod <- glmnet(x_train,y_train,alpha=0,lambda=best_ridge)
test_df$pred_ridge <- as.numeric(predict(ridge_mod,x_test))
summary(cv_ridge)

# Lasso
cv_lasso  <- cv.glmnet(x_train,y_train,alpha=1)
best_lasso<- cv_lasso$lambda.min
lasso_mod <- glmnet(x_train,y_train,alpha=1,lambda=best_lasso)
test_df$pred_lasso <- as.numeric(predict(lasso_mod,x_test))
summary(cv_lasso)

# RMSE
cat("Ridge RMSE:",  rmse(y_test, test_df$pred_ridge), "\n")
cat("Lasso RMSE:",  rmse(y_test, test_df$pred_lasso), "\n")
```

We do a Cross Validation of Ridge and Lasso regression to see which one is more accurate. As we can see, Ridge regression has a lower RMSE which is more accurate for our model, so we will plot the Cross validation curve. We get the RSME score from the Metrics Package.

### Plotting Ridge regression CV plot

```{r}
plot(cv_ridge)
abline(v=log(best_ridge),col="red",lty=2)
```
The cross validation ridge plot is shown above. According to "<https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/choosing-lambda.html>":

"What is plotted is the estimated CV MSE for each value of (log)lambda on the x-axis. The dotted line on the far left indicates the value of lambda which minimizes CV error. The dotted line roughly in the middle of the x-axis indicates the 1-standard-error lambda- recall that this is the maximum value that lambda can take while still falling within the on standard error interval of the minimum-CV lambda. The second line of code has manually added a dot-dash horizontal line at the upper end of the 1-standard deviation interval of the MSE at the minimum-CV lambda to illustrate this point further". These plots can change with randomization according to our seed number.


### Plotting our Comparison graph between MLR and Ridge Regression MLR

```{r}
plot(test_df$pred_ols, test_df$TD,
     xlim = range(c(test_df$pred_ols,test_df$pred_ridge)),
     ylim = range(c(test_df$pred_ols,test_df$pred_ridge)),
     xlab="Predicted TD", ylab="Actual TD",
     main="OLS (blue) vs Ridge (red)", pch=19, col=rgb(0,0,1,0.6))
points(test_df$pred_ridge, test_df$TD, pch=17, col=rgb(1,0,0,0.6))
abline(0,1,lty=2)
legend("topleft", legend=c("OLS","Ridge"), pch=c(19,17),
       col=c("blue","red"), bty="n")
```

We can compare our MLR Ordinary Least Squares Regression Model with our Cross-Validated, Ridge Regression Model visually as shown above.

### Summary of trained and tested data metrics
```{r}
summary(train_df)
summary(test_df)
```

The summary shown by the trained/tested data are regularized and explain the scale of the variables within the ridge regression. We can use the test dataframe metrics to find the ideal candidate for the Atlanta Falcons on the offensive side of the ball.

### Conclusion of Findings

We can now safely say that the Atlanta Falcons TDs can be predicted by multiple factors within a game such as catch percentage, receptions, yards, and other numerical factors.

We see that an ideal candidate for the Atlanta Falcons on the offensive side of the ball
(particularly WRs, TEs and RBs) would have the optimal stats of:

Around 39 Receiving targets (Based on Median) (Tgt)

Approximately 63 Receptions (Based on IQR) (Rec)

62% catch percentage (based on Median) (Ctch%)

876 - 1500** yards (IQR and mean) (Yds)

** **(note that the mean is not used as a measure of spread here, but rather a range of indication for players with the IQR fitting the offensive scheme of the falcons)** **.

All of these stats would be preferred over a 69 game span (IQR) according to our model, which would be about 4 full seasons in the NFL (69 games from IQR divided by a 17 game NFL season in the modern era equals about 4 full seasons).This does not mean that the player should get these stats within 4 seasons, but rather, the player must **consistently** achieve these ranges of metrics for 4 **consecutive** seasons in order to be a good fit.

This means the player would have to be 25-26 years old to be considered for the Atlanta Falcons.

We can assume that a combination of these stats (with slight variability based on outliers with superstar potential) will lead to a productive increase (or stability in case of outliers) in Touchdowns for the Atlanta Falcons in the case of picking up free agents, resigning players, or trading for talent.

Keep in mind that these stats are based on my personal interpretation and can vary from person to person. I have used data online and interpreted the Falcons offensive scheme from Zac Robinson's (Falcons Offensive Coordinator) Air-Raid philosophy (based on what I've found online).



### Future improvements

1. Automating roster/free agent/trade/recruiting data in future findings

2. Creating a classification model detailing other external factors (behavior, team chemistry, etc. modeled for binary classification (0 or 1) or other techniques) can also be used in tandem with this model in order to make an even more accurate decision.

3. Expanding the model to look at more advanced offensive stats/metrics like Y/G, Y/Tgt, etc.

4. Making an extensive ML regression workflow to determine team-fit with Free Agent data, NFL Trade data, or College NCAA data for drafts (NCAA data would have to be adjusted to NFL standards for accurate comparison specifically for the Falcons).

5. Creating multiple models and compare test statistics to figure out which results are more tangible to use based on directions from team scouts, front offices, coaches, etc.


### References Used:

<https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/choosing-lambda.html>

<https://www.pro-football-reference.com/teams/atl/career-receiving.htm>

<https://online.stat.psu.edu/stat462/node/180/>

<https://stats.stackexchange.com/questions/279300/how-to-interpret-cross-validation-plot-from-glmnet>

<https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression>

<https://online.stat.psu.edu/stat462/node/131/>

<https://www.reddit.com/r/AskStatistics/comments/ycjoy4/what_threshold_is_used_to_assess_multicollinearity/>

<https://www.datacamp.com/doc/r/regression>

<https://www.geo.fu-berlin.de/en/v/soga-r/Basics-of-statistics/Linear-Regression/Polynomial-Regression/Polynomial-Regression---An-example/index.html>

<https://online.stat.psu.edu/stat462/node/177/>

<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html>

<https://www.reddit.com/r/rstats/comments/oibm2w/how_to_deal_with_multicollinearity_in_linear/>

<https://www.datacamp.com/tutorial/multicollinearity>

<https://www.rdocumentation.org/packages/car/versions/3.1-3>

<https://www.rdocumentation.org/packages/Metrics/versions/0.1.4>

<https://www.r-bloggers.com/2021/10/lambda-min-lambda-1se-and-cross-validation-in-lasso-binomial-response/>

<https://www.techtarget.com/searchenterpriseai/definition/data-splitting#:~:text=Training%20sets%20are%20commonly%20used,the%20final%20model%20works%20correctly.>

<https://www.datacamp.com/doc/r/scatterplot-in-r>

<https://www.educba.com/multiple-linear-regression-in-r/>

<https://www.si.com/nfl/falcons/inside-zac-robinson-s-falcons-offense-speed-shifts-controlled-chaos-01hxvw1caw08?>

<https://apexfantasyleagues.com/peak-age-for-nfl-wide-receiver/>

<https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html#:~:text=variance_inflation_factor-,statsmodels.stats.outliers_influence.,wikipedia.org/wiki/Variance_inflation_factor>

